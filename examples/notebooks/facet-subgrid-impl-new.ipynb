{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facetted subgrids - Implementation\n",
    "\n",
    "This notebook is about implementation of the algorithm sketched out in [facet-subgrid.ipynb](facet-subgrid.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pylab\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.path as path\n",
    "\n",
    "from ipywidgets import interact, interact_manual\n",
    "import numpy\n",
    "import sys\n",
    "import random\n",
    "import itertools\n",
    "import time\n",
    "import scipy.special\n",
    "import math\n",
    "import tikzplotlib\n",
    "\n",
    "sys.path.append('../..')\n",
    "from crocodile.synthesis import *\n",
    "from util.visualize import *\n",
    "\n",
    "# Helper for marking ranges in a graph\n",
    "def mark_range(lbl, x0, x1=None, y0=None, y1=None, ax=None):\n",
    "    if ax is None: ax = pylab.gca()\n",
    "    if y0 is None: y0 = ax.get_ylim()[1]\n",
    "    if y1 is None: y1 = ax.get_ylim()[0]\n",
    "    wdt = ax.get_xlim()[1] - ax.get_xlim()[0]\n",
    "    ax.add_patch(patches.PathPatch(patches.Path([(x0,y0), (x0,y1)]), linestyle=\"--\"))\n",
    "    if x1 is not None:\n",
    "        ax.add_patch(patches.PathPatch(patches.Path([(x1,y0), (x1,y1)]), linestyle=\"--\"))\n",
    "    else:\n",
    "        x1 = x0\n",
    "    if pylab.gca().get_yscale() == 'linear':\n",
    "        lbl_y = (y0*7+y1) / 8\n",
    "    else: # Some type of log scale\n",
    "        lbl_y = (y0**7*y1)**(1/8)\n",
    "    ax.annotate(lbl, (x1+wdt/200, lbl_y))\n",
    "\n",
    "def mk_tikz(out):\n",
    "    tikzplotlib.save(out, axis_height='3.5cm', axis_width='.45\\\\textwidth', textsize=5,\n",
    "                     show_info=False, externalize_tables=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = 16, 4\n",
    "pylab.rcParams['image.cmap'] = 'viridis'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing parameters\n",
    "\n",
    "There are a lot of parameters involved in facet-subgrid recombination, with quite a few trade-offs involved. Yet in the end, it fundamentally boils down to three concerns:\n",
    "\n",
    "1. Precision from our outputs\n",
    "2. Granularity of final and intermediate data - e.g. size of facets/subgrids, but also intermediate data\n",
    "3. Overhead of the recombination - decides the amount of I/O required, also plays into overall efficiency\n",
    "\n",
    "The way we are going to approach this is to see precision and granularity as given, then work out the solution with the least overhead from there.\n",
    "\n",
    "This first requires a model for what parameter we need to reach a certain precision. We start with generating a bunch of candidate prolate-spheroidal wave functions that we might use as windowing functions. We are going to use the grid-space support $W$ to characterise them (=$x_n/y_n$ later), as this is what mostly determines our error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_steps = 32\n",
    "Ws = numpy.arange(4,22,1/W_steps)\n",
    "res = 1024\n",
    "alpha = 0\n",
    "normal = numpy.prod(numpy.arange(2*alpha-1,0,-2, dtype=float)) # double factorial\n",
    "pswfs = { W: anti_aliasing_function(res, alpha, numpy.pi*W/2).real / normal\n",
    "          for W in Ws }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for W in sorted(pswfs):\n",
    "    pylab.semilogy(coordinates(res)*res, pswfs[W])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base errror\n",
    "\n",
    "Now we figure out the 'base error' for every value of $W$, i.e. the amount of error we would expect to bleed out into the regions masked by $m$. We are going by the \"worst case\", which happens if:\n",
    "1. $y_n=\\frac 12y_G$ (i.e. facet covers half the image - note that `res = 2yG` below)\n",
    "2. $x_A=\\frac 12x_G$ (i.e. subgrid covers half the grid)\n",
    "3. $y_\\text{source} = y_n$ (i.e. test pattern assumes source exactly at facet border)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling is in grid space!\n",
    "def test_pattern(ov = 2):\n",
    "    # Pattern for source at 1/4\n",
    "    pattern = pad_mid(numpy.tile([1,1j,-1,-1j], ov*res//8), ov*res)\n",
    "    # Make symmetric\n",
    "    assert pattern[ov*res//4] == 1\n",
    "    assert pattern[3*ov*res//4] == 0\n",
    "    pattern[ov*res//4] = 0.5\n",
    "    pattern[3*ov*res//4] = 0.5\n",
    "    return fft(pattern)\n",
    "def test_pswf(W, ov = 2):\n",
    "    n = pad_mid(fft(pad_mid(ifft(pswfs[W]), ov*res // 2)), ov*res)\n",
    "    n[ov*res//4] /= 2\n",
    "    n[3*ov*res//4] = n[ov*res//4]\n",
    "    return n\n",
    "ov = 2\n",
    "pylab.plot(coordinates(ov*res)*ov*res, test_pattern(ov).real/res/ov);\n",
    "pylab.plot(coordinates(ov*res)*ov*res, test_pswf(12, ov).real);\n",
    "mark_range('$y_{source}=.5 y_G$', ov*res//4);\n",
    "pylab.legend(['$F[AG]$']); pylab.grid(); pylab.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In image space, $AG$ is the test source convolved with a sinc corresponding to the $A$ mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ov = 4\n",
    "FAG = test_pattern(ov)\n",
    "FA = fft(pad_mid(numpy.tile([1], ov*res//2+1), ov*res))\n",
    "def base_err_simple(W, show=False):\n",
    "    n = test_pswf(W, ov)\n",
    "    nAG = ifft(n * FAG)\n",
    "    base_err = numpy.sqrt(numpy.mean(numpy.abs(nAG[:res*ov//4 - int(numpy.ceil(W))])**2))\n",
    "    if show:\n",
    "        nA = ifft(n*FA)\n",
    "        pylab.semilogy(coordinates(ov*res), numpy.abs(ifft(n)), label='n');\n",
    "        pylab.semilogy(coordinates(ov*res), numpy.abs(ifft(FAG)), label='AG');\n",
    "        pylab.semilogy(coordinates(ov*res), numpy.abs(nA), label='n*A');\n",
    "        xAxN = 1/4 + numpy.ceil(W) / res / ov\n",
    "        mark_range('$x_A$', -1/4, 1/4);\n",
    "        mark_range('$x_A+x_N$', -xAxN, xAxN);\n",
    "        pylab.grid(); pylab.ylim(numpy.abs(nA[0])/10, 5);\n",
    "        pylab.legend(); pylab.show();\n",
    "        print(f'base error W={W}: {base_err}')\n",
    "    return base_err\n",
    "err_base = { W : base_err_simple(W, W == 12) for W in Ws }\n",
    "pylab.axline((Ws[0], err_base[Ws[0]]), (Ws[-1], err_base[Ws[-1]]), c='gray', ls=':')\n",
    "pylab.semilogy(Ws, [ err_base[W] for W in Ws ]);\n",
    "pylab.xlim(Ws[0],Ws[-1]); pylab.ylabel('base error'); pylab.xlabel('$W=x_n/y_n$'); pylab.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base error is given by the average signal strenght left outside the $x_A+x_n$ region. Note that the position of the source in the grid ($G$) indeed makes a difference here: $n*AG$ settles multiple orders of magnitude higher than $n*A$ (which would be equivalent to a source in the centre).\n",
    "\n",
    "Note that when we vary $W$, the base error evolves basically in a log-linear fashion.\n",
    "\n",
    "### Facet margins\n",
    "\n",
    "The base error tells us how much error we have to expect when we actually crop to $x_A+x_n$ size. This error will get amplified once we apply the inverse convolution $b$. As $n$ falls to zero quickly at certain points in image space, $b$ will get very large, amplifying the base error significantly.\n",
    "\n",
    "This can again be derived directly from the window function - for any target error we just need to figure out how much extra allowance we can afford, then find where the inverse of the convolution reaches that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_x_sorted_smooth(xs, ys, y):\n",
    "    assert(len(xs) == len(ys))\n",
    "    pos = numpy.searchsorted(ys, y)\n",
    "    if pos <= 0:\n",
    "        return xs[0]\n",
    "    if pos >= len(ys) or ys[pos] == ys[pos-1]:\n",
    "        return xs[len(ys)-1]\n",
    "    w = (y - ys[pos-1]) / (ys[pos] - ys[pos-1])\n",
    "    return xs[pos-1] * (1-w) + xs[pos] * w\n",
    "def find_x_sorted_logsmooth(xs, ys, y):\n",
    "    return find_x_sorted_smooth(xs, numpy.log(numpy.maximum(1e-100, ys)), numpy.log(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_x0(W, err, dim=1):\n",
    "    err /= err_base[W]; target = 1/err**(1/dim)\n",
    "    return find_x_sorted_logsmooth(-coordinates(res)[:res//2], pswfs[W][:res//2], target)\n",
    "find_x0(12,1e-5) # 0.3632584490529124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for W in numpy.arange(4, 16):\n",
    "    err = numpy.exp(-numpy.arange(0, 10, 0.125/4) * numpy.log(10))\n",
    "    pylab.semilogx(err, [ find_x0(W, e) for e in err ], label=\"W=%d\"%W)\n",
    "pylab.xlabel('Error'); pylab.ylabel('$y_B/y_n$'); pylab.legend(); pylab.ylim(-0.01, 0.51); pylab.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for W in numpy.arange(4, 16):\n",
    "    err = numpy.exp(-numpy.arange(0, 10, 0.125/2) * numpy.log(10))\n",
    "    pylab.semilogx(err, [ find_x0(W, e, 2) for e in err ], label=\"W=%d\"%W)\n",
    "pylab.xlabel('Error'); pylab.ylabel('$x_0$ (approx)'); pylab.legend(); pylab.ylim(-0.01, 0.51);\n",
    "pylab.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgrid margins 1: Decompression \n",
    "\n",
    "To enable handling subgrids without the need to solve grid-sized Fourier transforms, the subgrid mask must itself have a representation that has bounds in image space. What we are looking for is exactly the same properties as with the signal windowing function, therefore we simply use the same one.\n",
    "\n",
    "Theoretically we would reason that $n*AG$ needs $x_A+x_n$ margins, therefore $(\\hat{m}*n)(n*AG)$ would require $x_A+2x_n$. However, clearly $n*AG$ is very small close to the $x_A+x_n$ bounds, therefore we can actually choose the bounds somewhat smaller without introducing too much error. As we are essentially multiplying two shifted $n$, we can approximate the required margin by determining the point where $n*n$ (i.e. the window function convolved with itself) reaches the target error. We will call that point $x_{n^2}$.\n",
    "\n",
    "As this error will appear similarly as the truncation error from $n$ we will aim for the same base error as determined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2s = []\n",
    "ov = 2\n",
    "xA = 0.25\n",
    "FA = numpy.sinc(coordinates(res*ov)*xA*2*res*ov)*xA*2*res*ov\n",
    "FAG = test_pattern(ov)\n",
    "for W in Ws:\n",
    "    pswf = test_pswf(W,ov)\n",
    "    xN2 = 2*find_x_sorted_logsmooth(-coordinates(res*ov)[:res*ov//2], numpy.abs(ifft(pswf)**2)[:res*ov//2],\n",
    "                                    err_base[W])\n",
    "    if W == 23:\n",
    "        pylab.subplot(121)\n",
    "        Fm = numpy.sinc(coordinates(res*ov)*((xA+xN2)*2*res*ov))*((xA+xN2)*2*res*ov)\n",
    "        Fm1 = -Fm; Fm1[res*ov//2] += res*ov; # i.e. Fm1 = fft(1-ifft(Fm))\n",
    "        pylab.axline((pylab.gca().get_xlim()[0], err_base[W]), (pylab.gca().get_xlim()[1], err_base[W]),\n",
    "                     c='black', linestyle=':', linewidth=1)\n",
    "        pylab.semilogy(coordinates(res*ov), numpy.abs(ifft(Fm*pswf)), label='$\\hat m*n$')\n",
    "        pylab.semilogy(coordinates(res*ov), numpy.abs(ifft(Fm1*pswf)),\n",
    "                       label='$(1-\\hat{m})*n$')\n",
    "        pylab.semilogy(coordinates(res*ov), numpy.abs(ifft(pswf*FA)), label='$n*A$')\n",
    "        pylab.semilogy(coordinates(res*ov), numpy.abs(ifft(Fm1*pswf)*ifft(pswf*FA)),\n",
    "                       label='$((1-\\hat m)*n)(n*A)$')\n",
    "        pylab.semilogy(coordinates(res*ov), numpy.abs(ifft(pswf*FAG)), label='$n*AG$')\n",
    "        pylab.semilogy(coordinates(res*ov), numpy.abs(ifft(Fm1*pswf)*ifft(pswf*FAG)),\n",
    "                       label='$((1-\\hat m)*n)(n*AG)$')\n",
    "        xN = W / res / ov\n",
    "        mark_range('$x_A$', xA); mark_range('$x_A+x_n$', xN+xA); mark_range('$x_A+x_{n^2}$', xN2+xA)\n",
    "        mark_range('', xN2/2+xA); pylab.xlim(xA-0.05,xA+xN2+0.05); pylab.ylim(err_base[W]**1.3/1e2, 2e0);\n",
    "        pylab.title(f'subgrid mask border for W={W} (base={err_base[W]:.3g} $x_{{n^2}}/x_n={xN2/xN:.3f}$)');\n",
    "        pylab.legend(loc='center left'); pylab.grid();\n",
    "    W2s.append(xN2 * res * ov)\n",
    "pylab.subplot(122)\n",
    "pylab.plot(Ws, W2s);\n",
    "slope = (W2s[-1] - W2s[0]) / (Ws[-1] - Ws[0]) \n",
    "print( numpy.average(W2s - slope * Ws), \"+W*\", slope )\n",
    "pylab.axline((Ws[0], W2s[0]), (Ws[-1], W2s[-1]), c='black', linestyle=':')\n",
    "pylab.ylabel('$x_{n^2}/x_n$'); pylab.title('extra border required'); pylab.xlabel('W')\n",
    "W2s_map = { W: W2 for W, W2 in zip(Ws, W2s)};\n",
    "mk_tikz('masking.tikz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both error terms $((1-\\hat{m})*n)(n*A)$ and  $((1-\\hat{m})*n)(n*AG)$ (where $G$ is again the test pattern form above) show a clear maximum at $x_A+\\frac 12x_{n^2}$. Note that $x_{n^2}/x_n$ depends on $W$, but reaches $\\sim 1.6$ at worst.\n",
    "\n",
    "### Subgrid margins 2: Non-coplanarity\n",
    "\n",
    "In order to correct for array non-coplanarity, we might want to add allowances for adding Fresnel terms after recombination. Those are given as:\n",
    "\n",
    "$$G_w(l) = \\mathcal F_l\\left[ e^{2\\pi i w \\sqrt{1 - l^2}} \\right]$$\n",
    "\n",
    "Assuming that the maximum derivative of $w \\sqrt{1 - l^2}$ is given as $h$ (for hardness), and the maximum $l$ value is given by $theta$ (maximum field of view). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(ymax=(0,1,0.05), yN=(0,1,0.05), yG=(0,1,0.1), w=(0,1000, 1), W=(4,22-1/W_steps,1/W_steps))\n",
    "def test_noncoplanar(ymax=0.1, yN=0.1, yG=0.5, w=600, W=12):\n",
    "    pylab.clf()\n",
    "    for iw,W_ in enumerate(Ws):\n",
    "        if W - W_ < 1e-5:\n",
    "            break\n",
    "    W = W_\n",
    "    pswf = pswfs[W]\n",
    "    N = int(res*yG/yN); xG = N / yG / 4    \n",
    "    xN = W / 4 / yN; xN2 = W2s[iw] / 4 / yN; ys = yG*2*coordinates(N)+ymax-yN; xs = xG*2*coordinates(N)   \n",
    "    xgw = w*ymax/numpy.sqrt(1-ymax**2)\n",
    "    xgw_mid = w*(ymax-yN)/numpy.sqrt(1-(ymax-yN)**2)\n",
    "    print('xN=', xN, ' xN2=', xN2, ' x_gw=', xgw, ' N=', N)\n",
    "    n = pad_mid(pswf, N)\n",
    "    g = pad_mid(numpy.exp(2j*numpy.pi*w*numpy.sqrt(1-extract_mid(ys, res)**2)), N)\n",
    "    N0 = int(numpy.ceil(xgw_mid / xG / 2 * N + N // 2))\n",
    "    xNG = find_x_sorted_logsmooth(xs[:N0:-1], numpy.abs(ifft(g*n))[:N0:-1], err_base[W])\n",
    "    xNG2 = find_x_sorted_logsmooth(xs[:N0:-1], numpy.abs(ifft(g*n*n))[:N0:-1], err_base[W])\n",
    "    print('xNG=', xNG, ' xNG2=', xNG2)\n",
    "    pylab.semilogy(xs,numpy.abs(ifft(n)), label='n')\n",
    "    pylab.semilogy(xs,numpy.abs(ifft(n*n)), label='n*n')\n",
    "    pylab.semilogy(xs,numpy.abs(ifft(g)), label='g')\n",
    "    pylab.semilogy(xs,numpy.abs(ifft(g*n)), label='g*n')\n",
    "    pylab.semilogy(xs,numpy.abs(ifft(g*n*n)), label='g*n*n')\n",
    "    pylab.legend()\n",
    "    pylab.xlim(-max(xNG,xgw)-2*xN, max(xNG,xgw)+2*xN); pylab.ylim(err_base[W]/1e3,1);\n",
    "    mark_range('$x_{n}$', -xN,xN); mark_range('$x_{n^2}$', -xN2,xN2)\n",
    "    mark_range('$x_{g}$', -xgw,xgw); mark_range('$x_{ng}$', -xNG,xNG)\n",
    "    mark_range('$x_{n^2g}$', -xNG2,xNG2)\n",
    "    pylab.axline((-xG, err_base[W]), (xG, err_base[W]), c='black', linestyle=':', linewidth=1);\n",
    "    mk_tikz('noncoplanarity.tikz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymax=0.2\n",
    "yG=0.5\n",
    "w=600\n",
    "_Ws = list(reversed(Ws[W_steps*2::W_steps*2]))\n",
    "for yN in [0.1, 0.2]:\n",
    "    for iw, W in enumerate(_Ws):\n",
    "        pswf = pswfs[W]\n",
    "        N = int(res*yG/yN); xG = N / yG / 4    \n",
    "        xN = W / 4 / yN; xN2 = W2s[iw] / 4 / yN; ys = yG*2*coordinates(N)+ymax-yN; xs = xG*2*coordinates(N)   \n",
    "        n = pad_mid(pswf, N)\n",
    "        xgws = []; xNGs = []; xNG2s = []\n",
    "        for w in numpy.arange(0, 2000, 10):\n",
    "            xgw = w*ymax/numpy.sqrt(1-ymax**2)\n",
    "            xgw_mid = w*(ymax-yN)/numpy.sqrt(1-(ymax-yN)**2)\n",
    "            g = pad_mid(numpy.exp(2j*numpy.pi*w*numpy.sqrt(1-extract_mid(ys, res)**2)), N)\n",
    "            N0 = int(numpy.ceil(xgw_mid / xG / 2 * N + N // 2))\n",
    "            xNG = find_x_sorted_logsmooth(xs[:N0:-1], numpy.abs(ifft(g*n))[:N0:-1], err_base[W])\n",
    "            xNG2 = find_x_sorted_logsmooth(xs[:N0:-1], numpy.abs(ifft(g*n*n))[:N0:-1], err_base[W])\n",
    "            xgws.append(xgw); xNGs.append(xNG); xNG2s.append(xNG2)\n",
    "        pylab.plot(xgws, xNG2s - numpy.array(xgws), color=f'C{iw}', lw=yN*8)\n",
    "pylab.xlim(0, xgw); pylab.grid(); pylab.legend([f'W={W}' for W in _Ws]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, for choosingthe granularity of the data we are mostly constrained by the desired facet and subgrid size. Except we generally do not actually care *that* much about its exact values, especially for facets: We just want them to be a certain order of magnitude so we can work with them within the memory constraints of a node or a CPU's cache.\n",
    "\n",
    "The point in our algorithm where we do care a lot about the exact value is whenever we solve a Fast Fourier Transform. These transforms are clearly at their most efficient if the sizes decompose into only small primes. In the ideal case simply a power of 2. As it turns out, we will need to solve FFTs in three sizes:\n",
    "\n",
    "1. $2y_P$\n",
    "2. $4x_My_P$\n",
    "3. $2x_MN$\n",
    "\n",
    "Therefore we have to only choose these three values appropriately in order to get optimal FFT performance. As we now are closer to the implementation side, let's just start with the sizes we would like to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_err = 1e-5\n",
    "\n",
    "N = 1024 \n",
    "yP_size = 512\n",
    "xM_size = 256\n",
    "#N = 128 * 1024\n",
    "#yP_size = 32 * 1024\n",
    "#xM_size = 512\n",
    "fov = 2 * 0.375\n",
    "\n",
    "assert(xM_size * yP_size % N == 0)\n",
    "\n",
    "xE = 0 # extra x space, e.g. for w-kernel\n",
    "\n",
    "assert(N % xM_size == 0)\n",
    "xM_step = N // xM_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are actually all the constraints we need - N tells us the total image size, `xM_size` the rough size of subgrids, and `yP_size` the rough size of facets. No we \"just\" need to figure out what actual configurations are viable.\n",
    "\n",
    "First we need to keep in mind that we need margins both in grid and image space to make this work. We need:\n",
    "$$x_A < x_M - 2x_N$$\n",
    "So we need to leave enough margin in grid space to fit the gridding function $n$ twice - once for actual convolution, and then again to allow us to short-cut the $m$ mask operation. Furthermore we require:\n",
    "$$y_B < 2y_P - 2y_N$$\n",
    "To leave enough room so we don't get aliasing when masking $m$. Also remember that $y_N$ here is $y_B$ padded to make gridding accurate, so with the nomenclature from above:\n",
    "$$y_N = \\frac{y_B}{2x_0} \\quad\\Rightarrow\\quad y_B < \\frac{2y_P}{1+ \\frac 1{x_0}}$$\n",
    "\n",
    "To tie everything together, note that $x_N = \\frac{R}{y_N}$. From here we can calculate an \"ideal\" overhead value simply by comparing how much we have lost with $x_A$ compared to $x_M$ (depends on $R$) and $y_B$ compared with $y_N$ (depends on $x_0$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_param_bounds(N, yP_size, xM_size, W, err, dim=1):\n",
    "    #x0 = approx_x0(W/2, err)\n",
    "    x0 = max(0.001, find_x0(W, err, dim))\n",
    "    yB_size_m = 2 * yP_size / (1+1/x0)\n",
    "    yN_size_m = yB_size_m / 2 / x0\n",
    "    xN_size = W/yN_size_m*N\n",
    "    xN2_size = W2s_map[W] / yN_size_m * N\n",
    "    xA_size_m = xM_size - xE - xN2_size\n",
    "    if xA_size_m > 0:\n",
    "        base_ov = yN_size_m * xM_size / xA_size_m / yB_size_m\n",
    "    else:\n",
    "        base_ov = None\n",
    "    return x0, xN_size, yN_size_m, xA_size_m, yB_size_m, base_ov\n",
    "\n",
    "for err in 10.**numpy.arange(-9,-2):\n",
    "    b_overheads = [ calc_param_bounds(N, yP_size, xM_size, W, err)[-1] for W in Ws ]\n",
    "    pylab.plot(Ws, b_overheads, label=\"err=%.0e\" % err)\n",
    "pylab.ylabel(\"Overhead\"); pylab.xlabel(\"W\"); pylab.ylim(1,10); pylab.legend(); pylab.show();\n",
    "for err in 10.**numpy.arange(-9,-2):\n",
    "    b_overheads = [ calc_param_bounds(N, yP_size, xM_size, W, err)[0] for W in Ws ]\n",
    "    pylab.plot(Ws, b_overheads, label=\"err=%.0e\" % err)\n",
    "pylab.ylabel(\"$x_0$\");pylab.xlabel(\"W\"); pylab.legend(); pylab.ylim(0,0.5); pylab.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks nice and smooth, but this isn't the full story. To actually identify usable parameters for our algorithm to be efficient we want to apply simple coordinate shifts on down-sampled grids/images in order to position the contributions of subgrids/subimages. This *only* works if the facet and subgrid offsets are actually chosen such that they correspond to whole numbers even after down-sampling.\n",
    "\n",
    "In practice, this boils down to finding a suitable decomposition $N_x N_y = N$ with $N_x, N_y \\in \\mathbb{N}$ such that:\n",
    "\n",
    "1. $N_x$ divides $x_M$ and $x_A$ ($\\cdot 2N$)\n",
    "2. $N_y$ divides $y_B$, $y_N$ and $y_P$ ($\\cdot 2$)\n",
    "\n",
    "The reasons are relatively obvious for the sizes that correspond to the down-sampling we do (specifically $y_N$, $y_P$ and $x_M$).\n",
    "\n",
    "On the other hand, while $x_A$ and $y_B$ are \"just\" about the usable area of facets and subgrids, in practice that usable area is only really useful up to the same multiplicators. The reason is that we often want to cover a larger area with subgrids and/or subimages, which leads to a \"tiling\". However as $N_x$ and $N_y$ restrict the valid subimage / subgrid offsets we can use, we only have certain choices of spacings between those tiles. This means that we can only use subimage/subfacet area up to the greatest multiple or $N_x$/$N_y$ in a systematic way. By insisting on $x_A$ / $y_B$ be divisible we simply include this in our overhead calculation.\n",
    "\n",
    "Of the parameters given above, we have $N$, $x_M$ and $y_P$ given, so those simply restrict the possible values of $N_x$ and $N_y$. However, we have to identify $N_x$ and $N_y$ such that we get minimum overhead from adjusting $x_A$, $y_B$ and $y_N$ accordingly. It is easiest to simply try all options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_configuration(N, yP_size, xM_size, W, err, wtowers = False, fov = 1):\n",
    "    \"\"\" Find best imaging configuration\n",
    "    :param N: Total grid/imaeg size\n",
    "    :param yP_size: Padded facet size\n",
    "    :param xM_size: Padded subgrid size\n",
    "    :param W: PSWF parameter\n",
    "    :param wtowers: Use w-towers efficiency (3D) instead of plain efficiency (1D)?\n",
    "    :param fov: How how of the field of view is assumed to be required\n",
    "    :returns: Parameter dictionary\n",
    "    \"\"\"\n",
    "    assert(xM_size * yP_size % N == 0)\n",
    "    assert(N % xM_size == 0)\n",
    "\n",
    "    # Get parameter bounds, exit early if impossible\n",
    "    x0, xN_size, yN_size_m, xA_size_m, yB_size_m, base_eff = calc_param_bounds(N, yP_size, xM_size, W, err, dim=2)\n",
    "    if xA_size_m <= 0 and yB_size_m <= 0:\n",
    "        return None\n",
    "\n",
    "    #print(\"R=%g, x0=%g, yB_size<%g, yN_size<%g, xA_size<%g, ov=\" % (R, x0, yB_size_m, yN_size_m, xA_size_m), end='')\n",
    "    base_eff = 1 / (yN_size_m * xM_size / xA_size_m / yB_size_m)\n",
    "    base_eff_g = 1 / ((yN_size_m * xM_size / (xA_size_m * 2 / 3) / yB_size_m)**2 / (xA_size_m / 3))\n",
    "    best_eff = 1e-6; best_pars = None;\n",
    "    xM_step = N // xM_size\n",
    "    for Ny in xM_step * numpy.arange(1, N // xM_step):\n",
    "        if N % Ny != 0: continue\n",
    "        Nx = N // Ny\n",
    "        if Nx > xA_size_m or Ny > yB_size_m: continue\n",
    "        xA_size = Nx * int(xA_size_m // Nx) # round down\n",
    "        yB_size = Ny * int(yB_size_m // Ny) # round down\n",
    "        yN_size = xM_step * int((yN_size_m + xM_step - 1) // xM_step) # round up\n",
    "        # Rounding might (extremely rarely) cause us to violate the side condition\n",
    "        while yB_size / 2 + yN_size > yP_size:\n",
    "            yB_size -= Ny\n",
    "        if yB_size <= 0:\n",
    "            continue\n",
    "        # Determine how many facets we'll need to cover the field of view.\n",
    "        # By reducing the \"effective\" facet size we can represent the\n",
    "        # inefficiency from unused facet space\n",
    "        if fov is not None:\n",
    "            nfacet = int(numpy.ceil(N * fov / yB_size))\n",
    "            if nfacet % 2 == 0: nfacet += 1\n",
    "            yB_size_eff = N * fov / nfacet\n",
    "        else:\n",
    "            yB_size_eff = yB_size\n",
    "        base_pars = dict(\n",
    "            N=N, yP_size=yP_size, xM_size=xM_size, W=W, fov=fov,\n",
    "            x0=x0, Nx=Nx, Ny=Ny, xA_size=xA_size,\n",
    "            yB_size=yB_size, xN_size=xN_size, yN_size=yN_size\n",
    "        )\n",
    "        if not wtowers:\n",
    "            # Calculate efficiency\n",
    "            eff = 1 / (xM_size * yN_size / xA_size / yB_size_eff)\n",
    "            if eff > best_eff:\n",
    "                best_eff = eff;\n",
    "                best_pars = dict(eff=eff, base_eff=base_eff, **base_pars)\n",
    "        else:\n",
    "            # Calculate uvw efficiency\n",
    "            best_eff3_g = 1e-15; best_xA_size_g = None\n",
    "            for j in range(100):\n",
    "                xA_size_g = xA_size - j * Nx\n",
    "                if xA_size_g <= 0: break\n",
    "                if xA_size_g >= xA_size_m: continue\n",
    "                ov_g = 1 / ( (xM_size * yN_size / xA_size_g / yB_size_eff)**2 / (xA_size_m - xA_size_g))\n",
    "                if ov_g > best_eff3_g:\n",
    "                    best_eff3_g = ov_g; best_xA_size_g = xA_size_g\n",
    "            if best_eff3_g > best_eff:\n",
    "                best_eff = best_eff3_g\n",
    "                best_pars = dict(eff=best_eff3_g, base_eff=base_eff_g, xA_size_g=best_xA_size_g, **base_pars)\n",
    "\n",
    "    return best_pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot1, plot2 = pylab.figure(figsize=(16,8)).subplots(2)\n",
    "\n",
    "plot1.set_ylabel(\"$x_Ay_B(x_my_n)^{-1}$\")\n",
    "plot1.set_xlabel(\"$W = 4x_Ny_N$\")\n",
    "plot2.set_ylabel(\"$(x_A^2x_g)(y_B^2y_G)(x_my_n)^{-1}$\")\n",
    "plot2.set_xlabel(\"$W = 4x_Ny_N$\")\n",
    "\n",
    "mk_patch = lambda x, y, c: patches.Patch((x,y),eg=f'C{c}', fc=f'C{c}')\n",
    "\n",
    "\n",
    "for i, err in enumerate(numpy.exp(numpy.arange(-3,-10,-1)*numpy.log(10))):\n",
    "    #Rs = numpy.arange(2,15,1/64)\n",
    "    b_effectives = []; effectives = []; b_effectives_g = []; effectives_g = []\n",
    "    best_eff = 1e-6; best_pars = None; best_eff_g = 1e-6; best_pars_g = None\n",
    "    print(\"err=%g\" % err)\n",
    "    for W in Ws:\n",
    "\n",
    "        # Find best configuration - with wtowers and without\n",
    "        best_pars2 = find_best_configuration(N, yP_size, xM_size, W, err, fov=fov)\n",
    "        best_pars2_g = find_best_configuration(N, yP_size, xM_size, W, err, True, fov=fov)\n",
    "\n",
    "        if best_pars2 is not None and best_pars2['eff'] > best_eff:\n",
    "            best_eff = best_pars2['eff']; best_pars = best_pars2\n",
    "        if best_pars2_g is not None and best_pars2_g['eff'] > best_eff_g:\n",
    "            best_eff_g = best_pars2_g['eff']; best_pars_g = best_pars2_g\n",
    "        b_effectives.append(best_pars2['base_eff'] if best_pars2 is not None else 0)\n",
    "        effectives.append(best_pars2['eff'] if best_pars2 is not None else 0)\n",
    "        b_effectives_g.append(best_pars2_g['base_eff'] if best_pars2_g is not None else 0)\n",
    "        effectives_g.append(best_pars2_g['eff'] if best_pars2_g is not None else 0)        \n",
    "        \n",
    "    plot1.plot(Ws, b_effectives, linestyle=':', c = f'C{i}')\n",
    "    plot1.plot(Ws, effectives, label=\"error %g\" % err, c = f'C{i}')\n",
    "    plot2.plot(Ws, b_effectives_g, linestyle=':', c = f'C{i}')\n",
    "    plot2.plot(Ws, effectives_g, label=\"error %g\" % err, c = f'C{i}')\n",
    "\n",
    "    if best_pars is not None:\n",
    "        print((\" => eff={eff:.2f} (<{base_eff:.2f}),\\tW={W:.2f}, x0={x0:.2f}, Nx={Nx}, Ny={Ny},\"\n",
    "               \" xA_s={xA_size}, yB_s={yB_size}, xN_s={xN_size:.2f}, yN_s={yN_size}\").format(**best_pars) )\n",
    "        plot1.plot(best_pars['W'], best_pars['eff'], c = f'C{i}', marker='o')\n",
    "    if best_pars_g is not None:\n",
    "        print((\" => eff={eff:.2f} (<{base_eff:.2f}),\\tW={W:.2f}, x0={x0:.2f}, Nx={Nx}, Ny={Ny},\"\n",
    "               \" xA_s={xA_size_g}/{xA_size}, yB_s={yB_size}, xN_s={xN_size:.2f}, yN_s={yN_size}\").format(**best_pars_g) )\n",
    "        plot2.plot(best_pars_g['W'], best_pars_g['eff'], c = f'C{i}', marker='o')\n",
    "    if abs(err - target_err) < 1e-12:\n",
    "        target_pars = best_pars_g\n",
    "        print(\"^^^\")\n",
    "\n",
    "plot1.legend()\n",
    "plot1.set_ylim(1e-6,plot1.get_ylim()[1]); plot1.set_xlim(4, 22)\n",
    "plot1.grid();\n",
    "plot2.set_ylim(1e-6,plot2.get_ylim()[1]); plot2.set_xlim(4, 22)\n",
    "plot2.grid()\n",
    "\n",
    "for n in target_pars:\n",
    "    exec(f'{n} = target_pars[\"{n}\"]')\n",
    "mk_tikz(\"effective-%dk-%dk-%gk.tikz\" % (N // 1024, yP_size // 1024, xM_size / 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xN = xN_size / 2 / N\n",
    "xM = xM_size / 2 / N\n",
    "yN = yN_size / 2\n",
    "xA = xA_size / 2 / N\n",
    "yB = yB_size / 2\n",
    "print(\"xN=%g xM=%g yN=%g xNyN=%g xA=%g\" % (xN, xM, yN, xN*yN, xA))\n",
    "\n",
    "xM_yP_size = xM_size * yP_size // N\n",
    "xMxN_yP_size = xM_yP_size + int(2 * numpy.ceil(xN_size * yP_size / N / 2))\n",
    "assert((xM_size * yN_size) % N == 0)\n",
    "xM_yN_size = xM_size * yN_size // N\n",
    "def fmt(x):\n",
    "    if x >= 1024*1024 and (x % (1024*1024)) == 0:\n",
    "        return \"%dM\" % (x // 1024 // 1024)\n",
    "    if x >= 1024 and (x % 1024) == 0:\n",
    "        return \"%dk\" % (x // 1024)\n",
    "    return \"%d\" % x\n",
    "cfg_name = \"%s_%s_%s_%g\" % (fmt(N), fmt(yP_size), fmt(xM_size), target_err)\n",
    "print(\"xM_yP_size=%d, xMxN_yP_size=%d, xM_yN_size=%d\" % (xM_yP_size, xMxN_yP_size, xM_yN_size))\n",
    "if fov is not None:\n",
    "    nfacet = int(numpy.ceil(N * fov / yB_size))\n",
    "    print(f\"{nfacet}x{nfacet} facets for FoV of {fov} ({N * fov / nfacet / yB_size * 100}% efficiency)\")\n",
    "print(\"\\n%s: %d,%d,%d,%d,%d,%d,%d,%d\" % (cfg_name, N, Nx, yB_size, yN_size, yP_size, xA_size, xM_size, xMxN_yP_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_configuration_W(N, yP_size, xM_size, err, wtowers = False):\n",
    "    best_eff = 0; best_pars = None\n",
    "    for W in Ws:\n",
    "        best_pars2 = find_best_configuration(N, yP_size, xM_size, W, err, wtowers)\n",
    "        if best_pars2 is not None and best_pars2['eff'] > best_eff:\n",
    "            best_eff = best_pars2['eff']; best_pars = best_pars2\n",
    "    return best_pars\n",
    "\n",
    "for N_, yP_size_, xM_size_ in [\n",
    "    ( 64 * 1024,  8 * 1024, 512),\n",
    "    ( 64 * 1024, 16 * 1024, 256),\n",
    "    ( 96 * 1024, 12 * 1024, 512),\n",
    "    ( 96 * 1024, 24 * 1024, 256),\n",
    "    (128 * 1024, 16 * 1024, 512),\n",
    "    (128 * 1024, 32 * 1024, 256),\n",
    "    (256 * 1024, 32 * 1024, 512),\n",
    "    (512 * 1024, 64 * 1024, 512),\n",
    "    ( 64 * 1024, 12 * 1024, 512),\n",
    "    ( 64 * 1024, 24 * 1024, 256),\n",
    "    (128 * 1024, 24 * 1024, 512),\n",
    "    (128 * 1024, 48 * 1024, 256),\n",
    "    (256 * 1024, 48 * 1024, 512),\n",
    "    \n",
    "    #( 64 * 1024,  8 * 1024, 512),\n",
    "    #(128 * 1024, 16 * 1024, 512),\n",
    "    #(128 * 1024, 32 * 1024, 256),\n",
    "    #(256 * 1024, 32 * 1024, 512),    \n",
    "]:\n",
    "    best_pars = find_best_configuration_W(N_, yP_size_, xM_size_, err, True)\n",
    "    best_pars['eff'] *= 100\n",
    "    print('{N:,} & {yP_size:,} & {yN_size:,} & {yB_size:,} & {xM_size:,} & {xA_size:,} & {xA_size_g:,} & {eff:.1f}\\% \\\\'\n",
    "          .format(**best_pars).replace(',','\\,'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate actual PSWF to use\n",
    "\n",
    "Same as above, but this time we calculate it to the full required resolution (facet size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0\n",
    "pswf = anti_aliasing_function(yN_size, alpha, numpy.pi*2*xN*yN).real\n",
    "pswf /= numpy.prod(numpy.arange(2*alpha-1,0,-2, dtype=float)) # double factorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that PSWF indeed satisfies intended bounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = coordinates(N); fx = N*coordinates(N)\n",
    "n = ifft(pad_mid(pswf, N))\n",
    "pylab.semilogy(coordinates(4*int(xN_size))*4*xN_size/N,\n",
    "               extract_mid(numpy.abs(ifft(pad_mid(pswf, N))),4*int(xN_size)));\n",
    "pylab.legend([\"n\"]);\n",
    "mark_range(\"$x_n$\", -xN,xN); pylab.xlim(-2*int(xN_size)/N, (2*int(xN_size)-1)/N); pylab.show();\n",
    "pylab.semilogy(coordinates(yN_size)*yN_size, pswf); pylab.legend([\"$\\\\mathcal{F}[n]$\"]);\n",
    "mark_range(\"$y_B$\", -yB,yB); pylab.xlim(-N//2,N//2-1); mark_range(\"$y_n$\", -yN,yN); pylab.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAG = fft(pad_mid(numpy.ones(xA_size), N))\n",
    "@interact(l0= (-1,1,0.01), phi = (0, 2, 0.01), w_phi2 = (0,1000, 1))\n",
    "def test_noncoplanar(l0, phi, w_phi2):\n",
    "    w = w_phi2 / phi**2\n",
    "    print(\"w=\", w)\n",
    "    l0 = -phi * (N-yN_size) / 4 / N\n",
    "    print('l0=', l0)\n",
    "    FG = numpy.exp(2.j * numpy.pi * w * numpy.sqrt(1 - (l0 + coordinates(N) * phi)**2))\n",
    "    Fn = pad_mid(pswf, N)\n",
    "    FnG = Fn * FG\n",
    "    Gn = numpy.abs(ifft(FG * Fn))\n",
    "    pylab.semilogy(coordinates(N), numpy.abs(ifft(FG)))\n",
    "    pylab.semilogy(coordinates(N), numpy.abs(ifft(Fn)))\n",
    "    pylab.semilogy(coordinates(N), numpy.abs(Gn))\n",
    "    pylab.semilogy(coordinates(N), numpy.abs(ifft(Fn*FAG)))\n",
    "    pylab.semilogy(coordinates(N), numpy.abs(ifft(FnG*FAG)))\n",
    "    pylab.legend(['G', 'n', 'G*n', 'n*A', 'G*n*A'])\n",
    "    mark_range('W', -W/2/res,W/2/res)\n",
    "    mark_range('W', -W/2/res-1/4,W/2/res+1/4)\n",
    "    p0 = numpy.searchsorted(Gn[:res//2], err_base[W])\n",
    "    print(Gn[p0-1], err_base[W], Gn[p0])\n",
    "    p0 = (p0 * (err_base[W] - Gn[p0-1]) +\n",
    "          (p0 - 1) * (Gn[p0] - err_base[W])) / (Gn[p0]-Gn[p0-1])\n",
    "    p = res//2 - p0\n",
    "    #mark_range('p', -p/res,p/res)\n",
    "    #mark_range('p', -p/res-1/4,p/res+1/4)\n",
    "    print(p-W/2)\n",
    "    pylab.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact_manual\n",
    "\n",
    "@interact_manual\n",
    "def export_pswf(pswf_path = \"../../data/grid/T06_pswf_%s.in\" % cfg_name):\n",
    "    with open(pswf_path, \"w\") as f:\n",
    "        numpy.fft.ifftshift(pswf).tofile(f)\n",
    "    print(\"wrote %s\" % pswf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate actual work terms to use. We need both $n$ and $b$ in image space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fb = 1/extract_mid(pswf, yB_size)\n",
    "Fn = pswf[(yN_size//2)%int(1/2/xM)::int(1/2/xM)]\n",
    "facet_m0_trunc = pswf * numpy.sinc(coordinates(yN_size)*xM_size/N*yN_size)\n",
    "facet_m0_trunc = xM_size*yP_size/N * extract_mid(ifft(pad_mid(facet_m0_trunc, yP_size)), xMxN_yP_size).real\n",
    "pylab.semilogy(coordinates(xMxN_yP_size)/yP_size*xMxN_yP_size, facet_m0_trunc); mark_range(\"xM\", -xM, xM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAG = fft(pad_mid(numpy.ones(xA_size), N))\n",
    "@interact(max_l0= (0,1,0.01), phi = (0, 2, 0.01), w = (0,1000, 1))\n",
    "def test_noncoplanar(max_l0, phi, w):\n",
    "    FG = numpy.exp(2.j * numpy.pi * w * numpy.sqrt(1 - (max_l0 + coordinates(yB_size) * phi)**2))\n",
    "    Fn = pad_mid(pswf, N)\n",
    "    FnG = Fn * FG\n",
    "    Gn = numpy.abs(ifft(FG * Fn))\n",
    "    pylab.semilogy(coordinates(N), numpy.abs(ifft(FG)))\n",
    "    pylab.semilogy(coordinates(N), numpy.abs(ifft(Fn)))\n",
    "    pylab.semilogy(coordinates(N), numpy.abs(Gn))\n",
    "    pylab.semilogy(coordinates(N), numpy.abs(ifft(Fn*FAG)))\n",
    "    pylab.semilogy(coordinates(N), numpy.abs(ifft(FnG*FAG)))\n",
    "    pylab.legend(['G', 'n', 'G*n', 'n*A', 'G*n*A'])\n",
    "    mark_range('W', -W/2/res,W/2/res)\n",
    "    mark_range('W', -W/2/res-1/4,W/2/res+1/4)\n",
    "    p0 = numpy.searchsorted(Gn[:res//2], err_base[W])\n",
    "    print(Gn[p0-1], err_base[W], Gn[p0])\n",
    "    p0 = (p0 * (err_base[W] - Gn[p0-1]) +\n",
    "          (p0 - 1) * (Gn[p0] - err_base[W])) / (Gn[p0]-Gn[p0-1])\n",
    "    p = res//2 - p0\n",
    "    #mark_range('p', -p/res,p/res)\n",
    "    #mark_range('p', -p/res-1/4,p/res+1/4)\n",
    "    print(p-W/2)\n",
    "    pylab.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layout subgrids + facets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsubgrid = int(math.ceil(N / xA_size))\n",
    "nfacet = int(math.ceil(N / yB_size))\n",
    "print(\"%d subgrids, %d facets needed to cover\" % (nsubgrid, nfacet))\n",
    "subgrid_off = xA_size * numpy.arange(nsubgrid)\n",
    "facet_off = yB_size * numpy.arange(nfacet)\n",
    "def whole(xs): return numpy.all(numpy.abs(xs - numpy.around(xs)) < 1e-13)\n",
    "assert whole(numpy.outer(subgrid_off, facet_off) / N)\n",
    "assert whole(facet_off*xM_size/N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a bunch of array constants derived from the gridding function:\n",
    " * $\\mathcal Fb$ ($y_B$ size)\n",
    " * $\\mathcal Fn$ ($y_N$ size, sampled at $x_M$ rate), as well as \n",
    " * $\\mathcal Fm' = \\mathcal Fn\\mathcal Fm$ term ($y_P$ size, sampled at $x_M+x_N$).\n",
    " \n",
    "For the convolution with $b$, $n$, and cheap multiplication with $m$ at $y_P$ image size respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine subgrid/facet offsets and the appropriate A/B masks for cutting them out. We are aiming for full coverage here: Every pixel is part of exactly one subgrid / facet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgrid_A = numpy.zeros((nsubgrid, xA_size), dtype=int)\n",
    "subgrid_border = (subgrid_off + numpy.hstack([subgrid_off[1:],[N]])) // 2\n",
    "for i in range(nsubgrid):\n",
    "    left = (subgrid_border[i-1] - subgrid_off[i] + xA_size//2) % N\n",
    "    right = subgrid_border[i] - subgrid_off[i] + xA_size//2\n",
    "    assert left >= 0 and right <= xA_size, \"xA not large enough to cover subgrids!\"\n",
    "    subgrid_A[i,left:right] = 1\n",
    "\n",
    "facet_B = numpy.zeros((nfacet, yB_size), dtype=bool)\n",
    "facet_split = numpy.array_split(range(N), nfacet)\n",
    "facet_border = (facet_off + numpy.hstack([facet_off[1:],[N]])) // 2\n",
    "for j in range(nfacet):\n",
    "    left = (facet_border[j-1] - facet_off[j] + yB_size//2) % N\n",
    "    right = facet_border[j] - facet_off[j] + yB_size//2\n",
    "    assert left >= 0 and right <= yB_size, \"yB not large enough to cover facets!\"\n",
    "    facet_B[j,left:right] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the actual subgrids & facets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subgrid_and_facet(G):\n",
    "    FG = fft(G)\n",
    "    subgrid = numpy.empty((nsubgrid, xA_size), dtype=complex)\n",
    "    for i in range(nsubgrid):\n",
    "        subgrid[i] = subgrid_A[i] * extract_mid(numpy.roll(G, -subgrid_off[i]), xA_size)\n",
    "    facet = numpy.empty((nfacet, yB_size), dtype=complex)\n",
    "    for j in range(nfacet):\n",
    "        facet[j] = facet_B[j] * extract_mid(numpy.roll(FG, -facet_off[j]), yB_size)\n",
    "    return subgrid, facet\n",
    "subgrid, facet = make_subgrid_and_facet(numpy.random.rand(N)-0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facet $\\rightarrow$ Subgrid\n",
    "\n",
    "With a few more slight optimisations we arrive at a compact representation for our algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xN_yP_size = xMxN_yP_size - xM_yP_size\n",
    "def facets_to_subgrid_1(facet):\n",
    "    RNjMiBjFj = numpy.empty((nsubgrid, nfacet, xM_yN_size), dtype=complex)\n",
    "    for j in range(nfacet):\n",
    "        BjFj = ifft(pad_mid(facet[j] * Fb, yP_size))\n",
    "        for i in range(nsubgrid):\n",
    "            MiBjFj = facet_m0_trunc * extract_mid(numpy.roll(BjFj, -subgrid_off[i]*yP_size//N), xMxN_yP_size)\n",
    "            MiBjFj_sum = numpy.array(extract_mid(MiBjFj, xM_yP_size))\n",
    "            MiBjFj_sum[:xN_yP_size//2] += MiBjFj[-xN_yP_size//2:]\n",
    "            MiBjFj_sum[-xN_yP_size//2:] += MiBjFj[:xN_yP_size//2:]\n",
    "            RNjMiBjFj[i,j] = Fn * extract_mid(fft(MiBjFj_sum), xM_yN_size)\n",
    "    return RNjMiBjFj\n",
    "def facets_to_subgrid_2(nmbfs, i):\n",
    "    approx = numpy.zeros(xM_size, dtype=complex)\n",
    "    for j in range(nfacet):\n",
    "        approx += numpy.roll(pad_mid(nmbfs[i,j], xM_size), facet_off[j]*xM_size//N)\n",
    "    return subgrid_A[i] * extract_mid(ifft(approx), xA_size)\n",
    "\n",
    "print(\"Facet data:\", facet.shape, facet.size)\n",
    "nmbfs = facets_to_subgrid_1(facet)\n",
    "# - redistribution of nmbfs here -\n",
    "print(\"Redistributed data:\", nmbfs.shape, nmbfs.size, \" overhead:\", nmbfs.size / facet.size)\n",
    "approx_subgrid = [ facets_to_subgrid_2(nmbfs, i) for i in range(nsubgrid) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the error terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pylab.figure(figsize=(16, 8))\n",
    "ax1, ax2 = fig.add_subplot(211), fig.add_subplot(212)\n",
    "err_sum = err_sum_img = 0\n",
    "for i in range(nsubgrid):\n",
    "    error = approx_subgrid[i] - subgrid[i]\n",
    "    ax1.semilogy(xA*2*coordinates(xA_size), numpy.abs(error))\n",
    "    ax2.semilogy(N*coordinates(xA_size), numpy.abs(fft(error)))\n",
    "    err_sum += numpy.abs(error)**2 / nsubgrid\n",
    "    err_sum_img += numpy.abs(fft(error))**2 / nsubgrid\n",
    "mark_range(\"$x_A$\", -xA, xA, ax=ax1); mark_range(\"$N/2$\", -N/2, N/2, ax=ax2)\n",
    "print(\"RMSE:\", numpy.sqrt(numpy.mean(err_sum)), \"(image:\", numpy.sqrt(numpy.mean(err_sum_img)), \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By feeding the implementation single-pixel inputs we can create a full error map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if N <= 1024:\n",
    "    error_map = []\n",
    "    for xs in range(-N//2, N//2):\n",
    "        if xs % 128 == 0:\n",
    "            print(xs, end=' ')\n",
    "        FG = numpy.zeros(N); FG[xs + N//2] = 1\n",
    "        subgrid, facet = make_subgrid_and_facet(ifft(FG))\n",
    "        nmbfs = facets_to_subgrid_1(facet)\n",
    "        err_map_row = numpy.zeros(N, dtype=complex)\n",
    "        for i in range(nsubgrid):\n",
    "            error = facets_to_subgrid_2(nmbfs, i) - subgrid[i]\n",
    "            err_map_row += numpy.roll(pad_mid(error, N), subgrid_off[i])\n",
    "        error_map.append(fft(err_map_row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if N <= 1024:\n",
    "    err_abs = numpy.abs(error_map)\n",
    "    # Filter out spurious zeroes that would cause division-by-zero\n",
    "    err_log = numpy.log(numpy.maximum(numpy.min(err_abs[err_abs>0]), err_abs))/numpy.log(10)\n",
    "    pylab.figure(figsize=(20,20))\n",
    "    pylab.imshow(err_log, cmap=pylab.get_cmap('inferno'), norm=colors.PowerNorm(gamma=2.0),\n",
    "                 extent=(-N//2,N//2,-N//2,N//2));\n",
    "    pylab.colorbar(shrink=0.6); pylab.ylabel('in'); pylab.xlabel('out');\n",
    "    pylab.title('Output error depending on input pixel (absolute log10)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if N <= 1024:\n",
    "    worst_rmse = 0; worst_err = 0\n",
    "    for xs in range(N):\n",
    "        rmse = numpy.sqrt(numpy.mean(numpy.abs(error_map[xs])**2))\n",
    "        if rmse > worst_rmse: worst_rmse = rmse; worst_err = error_map[xs]\n",
    "    pylab.semilogy(numpy.abs(worst_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if N <= 1024:\n",
    "    worst_rmse = 0; worst_err = 0; em = numpy.array(error_map)\n",
    "    for xs in range(N):\n",
    "        rmse = numpy.sqrt(numpy.mean(numpy.abs(em[:,xs])**2))\n",
    "        if rmse > worst_rmse: worst_rmse = rmse; worst_err = em[:,xs]\n",
    "    pylab.semilogy(numpy.abs(worst_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subgrid $\\rightarrow$ facet\n",
    "\n",
    "The other direction works similarly, now we want:\n",
    "$$F_j \\approx b_j \\ast \\sum_i m_i (n_j \\ast S_i)$$\n",
    "\n",
    "We run into a very similar problem with $m$ as when reconstructing subgrids, except this time it happens because we want to construct:\n",
    "$$ b_j \\left( m_i (n_j \\ast S_i)\\right)\n",
    "  = b_j \\left( \\mathcal F^{-1}\\left[\\Pi_{2y_P} \\mathcal F m_i\\right] (n_j \\ast S_i)\\right)$$\n",
    "\n",
    "As usual, this is entirely dual: In the previous case we had a signal limited by $y_B$ and needed the result of the convolution up to $y_N$, whereas now we have a signal bounded by $y_N$, but need the convolution result up to $y_B$. This cancels out - therefore we are okay with the same choice of $y_P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgrid_to_facet_1(subgrid):\n",
    "    FNjSi = numpy.empty((nsubgrid, nfacet, xM_yN_size), dtype=complex)\n",
    "    for i in range(nsubgrid):\n",
    "        FSi = fft(pad_mid(subgrid[i], xM_size))\n",
    "        for j in range(nfacet):\n",
    "            FNjSi[i,j] = extract_mid(numpy.roll(FSi, -facet_off[j]*xM_size//N), xM_yN_size)\n",
    "    return Fn * FNjSi\n",
    "\n",
    "def subgrid_to_facet_2(nafs, j):\n",
    "    approx = numpy.zeros(yB_size, dtype=complex)\n",
    "    for i in range(nsubgrid):\n",
    "        NjSi = numpy.zeros(xMxN_yP_size, dtype=complex)\n",
    "        NjSi_mid = extract_mid(NjSi, xM_yP_size)\n",
    "        NjSi_mid[:] = ifft(pad_mid(nafs[i,j], xM_yP_size)) # updates NjSi via reference!\n",
    "        NjSi[-xN_yP_size//2:] = NjSi_mid[:xN_yP_size//2]\n",
    "        NjSi[:xN_yP_size//2:] = NjSi_mid[-xN_yP_size//2:]\n",
    "        FMiNjSi = fft(numpy.roll(pad_mid(facet_m0_trunc * NjSi, yP_size), subgrid_off[i]*yP_size//N))\n",
    "        approx += extract_mid(FMiNjSi, yB_size)\n",
    "    return approx * Fb * facet_B[j]\n",
    "\n",
    "print(\"Subgrid data:\", subgrid.shape, subgrid.size)\n",
    "nafs = subgrid_to_facet_1(subgrid)\n",
    "\n",
    "# - redistribution of FNjSi here -\n",
    "print(\"Intermediate data:\", nafs.shape, nafs.size)\n",
    "approx_facet = [ subgrid_to_facet_2(nafs, j) for j in range(nfacet) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pylab.figure(figsize=(16, 8))\n",
    "ax1, ax2 = fig.add_subplot(211), fig.add_subplot(212)\n",
    "err_sum = err_sum_img = 0\n",
    "for j in range(nfacet):\n",
    "    error = approx_facet[j] - facet[j]\n",
    "    err_sum += numpy.abs(ifft(error))**2\n",
    "    err_sum_img += numpy.abs(error)**2\n",
    "    ax1.semilogy(coordinates(yB_size), numpy.abs(ifft(error)))\n",
    "    ax2.semilogy(yB_size*coordinates(yB_size), numpy.abs(error))\n",
    "print(\"RMSE:\", numpy.sqrt(numpy.mean(err_sum)), \"(image:\", numpy.sqrt(numpy.mean(err_sum_img)), \")\")\n",
    "mark_range(\"$x_A$\", -xA, xA, ax=ax1); mark_range(\"$x_M$\", -xM, xM, ax=ax1)\n",
    "mark_range(\"$y_B$\", -yB, yB, ax=ax2); mark_range(\"$0.5$\", -.5, .5, ax=ax1)\n",
    "pylab.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_map_2 = []\n",
    "for xs in range(-N//2, N//2):\n",
    "    if xs % 128 == 0:\n",
    "        print(xs, end=' ')\n",
    "    FG = numpy.zeros(N); FG[xs + N//2] = 1\n",
    "    subgrid, facet = make_subgrid_and_facet(ifft(FG))\n",
    "    nafs = subgrid_to_facet_1(subgrid)\n",
    "\n",
    "    err_sum_hq = numpy.zeros(N, dtype=complex)\n",
    "    \n",
    "    for j in range(nfacet):\n",
    "        approx  = subgrid_to_facet_2(nafs, j)\n",
    "        err_sum_hq += numpy.roll(pad_mid(approx - facet[j], N), facet_off[j])\n",
    "    error_map_2.append(err_sum_hq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_abs = numpy.abs(error_map_2)\n",
    "# Filter out spurious zeroes that would cause division-by-zero\n",
    "err_log = numpy.log(numpy.maximum(numpy.min(err_abs[err_abs>0]), err_abs))/numpy.log(10)\n",
    "pylab.figure(figsize=(20,20))\n",
    "pylab.imshow(err_log, cmap=pylab.get_cmap('inferno'), norm=colors.PowerNorm(gamma=2.0),\n",
    "             extent=(-N//2,N//2,-N//2,N//2));\n",
    "pylab.colorbar(shrink=0.6); pylab.ylabel('in'); pylab.xlabel('out');\n",
    "pylab.title('Output error depending on input pixel (absolute log10)');\n",
    "tikzplotlib.save('error_map_2.tikz', axis_height='3.5cm', axis_width='\\\\textwidth', textsize=5, show_info=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_rmse = 0; worst_err = 0\n",
    "for xs in range(N):\n",
    "    rmse = numpy.sqrt(numpy.mean(numpy.abs(error_map_2[xs])**2))\n",
    "    if rmse > worst_rmse: worst_rmse = rmse; worst_err = error_map[xs]\n",
    "pylab.semilogy(numpy.abs(worst_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D case\n",
    "\n",
    "All of this generalises to two dimensions in the way you would expect. Let us set up test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nsubgrid,\"x\",nsubgrid,\"subgrids,\",nfacet,\"x\", nfacet,\"facets\")\n",
    "subgrid_2 = numpy.empty((nsubgrid, nsubgrid, xA_size, xA_size), dtype=complex)\n",
    "facet_2 = numpy.empty((nfacet, nfacet, yB_size, yB_size), dtype=complex)\n",
    "\n",
    "G_2 = numpy.exp(2j*numpy.pi*numpy.random.rand(N,N))*numpy.random.rand(N,N)/2\n",
    "FG_2 = fft(G_2)\n",
    "\n",
    "FG_2 = numpy.zeros((N,N))\n",
    "source_count = 1000\n",
    "sources = [ (numpy.random.randint(-N//2, N//2-1), numpy.random.randint(-N//2, N//2-1),\n",
    "             numpy.random.rand() * N * N / numpy.sqrt(source_count) / 2) for _ in range(source_count) ]\n",
    "for x, y, i in sources:\n",
    "    FG_2[y+N//2,x+N//2] += i\n",
    "G_2 = ifft(FG_2)\n",
    "print(\"Mean grid absolute:\", numpy.mean(numpy.abs(G_2)))\n",
    "\n",
    "for i0,i1 in itertools.product(range(nsubgrid), range(nsubgrid)):\n",
    "    subgrid_2[i0,i1] = extract_mid(numpy.roll(G_2, (-subgrid_off[i0], -subgrid_off[i1]), (0,1)), xA_size)\n",
    "    subgrid_2[i0,i1] *= numpy.outer(subgrid_A[i0], subgrid_A[i1])\n",
    "for j0,j1 in itertools.product(range(nfacet), range(nfacet)):\n",
    "    facet_2[j0,j1] = extract_mid(numpy.roll(FG_2, (-facet_off[j0], -facet_off[j1]), (0,1)), yB_size)\n",
    "    facet_2[j0,j1] *= numpy.outer(facet_B[j0], facet_B[j1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the amount of data has been squared, performance is a bit more of a concern now. Fortunately, the entire procedure is completely separable, so let us first re-define the operations to work on one array axis exclusively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_a(fill_val, axis_val, dims, axis):\n",
    "    return tuple([ axis_val if i == axis else fill_val for i in range(dims) ])\n",
    "def pad_mid_a(a, N, axis):\n",
    "    N0 = a.shape[axis]\n",
    "    if N == N0: return a\n",
    "    pad = slice_a((0,0), (N//2-N0//2, (N+1)//2-(N0+1)//2), \n",
    "                  len(a.shape), axis)    \n",
    "    return numpy.pad(a, pad, mode='constant', constant_values=0.0)\n",
    "def extract_mid_a(a, N, axis):\n",
    "    assert N <= a.shape[axis]\n",
    "    cx = a.shape[axis] // 2\n",
    "    if N % 2 != 0:\n",
    "        slc = slice(cx - N // 2, cx + N // 2 + 1)\n",
    "    else:\n",
    "        slc = slice(cx - N // 2, cx + N // 2)\n",
    "    return a[slice_a(slice(None), slc, len(a.shape), axis)]\n",
    "def fft_a(a, axis):\n",
    "    return numpy.fft.fftshift(numpy.fft.fft(numpy.fft.ifftshift(a, axis),axis=axis),axis)\n",
    "def ifft_a(a, axis):\n",
    "    return numpy.fft.fftshift(numpy.fft.ifft(numpy.fft.ifftshift(a, axis),axis=axis),axis)\n",
    "def broadcast_a(a, dims, axis):\n",
    "    slc = [numpy.newaxis] * dims\n",
    "    slc[axis] = slice(None)\n",
    "    return a[slc]\n",
    "def broadcast_a(a, dims, axis):\n",
    "    return a[slice_a(numpy.newaxis, slice(None), dims, axis)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to define the two fundamental operations - going from $F$ to $b\\ast F$ and from $b\\ast F$ to $n\\ast m(b\\ast F)$ separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_facet(facet, axis):\n",
    "    BF = pad_mid_a(facet * broadcast_a(Fb, len(facet.shape), axis), yP_size, axis)\n",
    "    BF = ifft_a(BF, axis)\n",
    "    return BF\n",
    "def extract_subgrid(BF, i, axis):\n",
    "    dims = len(BF.shape)\n",
    "    BF_mid = extract_mid_a(numpy.roll(BF, -subgrid_off[i]*yP_size//N, axis), xMxN_yP_size, axis)\n",
    "    MBF = broadcast_a(facet_m0_trunc,dims,axis) * BF_mid\n",
    "    MBF_sum = numpy.array(extract_mid_a(MBF, xM_yP_size, axis))\n",
    "    xN_yP_size = xMxN_yP_size - xM_yP_size\n",
    "    # [:xN_yP_size//2] / [-xN_yP_size//2:] for axis, [:] otherwise\n",
    "    slc1 = slice_a(slice(None), slice(xN_yP_size//2), dims, axis)\n",
    "    slc2 = slice_a(slice(None), slice(-xN_yP_size//2,None), dims, axis)\n",
    "    MBF_sum[slc1] += MBF[slc2]\n",
    "    MBF_sum[slc2] += MBF[slc1]\n",
    "    return broadcast_a(Fn,len(BF.shape),axis) * \\\n",
    "           extract_mid_a(fft_a(MBF_sum, axis), xM_yN_size, axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having those operations separately means that we can shuffle things around quite a bit without affecting the result. The obvious first choice might be to do all facet-preparation up-front, as this allows us to share the computation across all subgrids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "NMBF_NMBF = numpy.empty((nsubgrid, nsubgrid, nfacet, nfacet, xM_yN_size, xM_yN_size), dtype=complex)\n",
    "for j0,j1 in itertools.product(range(nfacet), range(nfacet)):\n",
    "    BF_F = prepare_facet(facet_2[j0,j1], 0)\n",
    "    BF_BF = prepare_facet(BF_F, 1)\n",
    "    for i0 in range(nsubgrid):\n",
    "        NMBF_BF = extract_subgrid(BF_BF, i0, 0)\n",
    "        for i1 in range(nsubgrid):\n",
    "            NMBF_NMBF[i0,i1,j0,j1] = extract_subgrid(NMBF_BF, i1, 1)\n",
    "print(time.time() - t, \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, remember that `prepare_facet` increases the amount of data involved, which in turn means that we need to shuffle more data through subsequent computations.\n",
    "\n",
    "Therefore it is actually more efficient to first do the subgrid-specific reduction, and *then* continue with the (constant) facet preparation along the other axis. We can tackle both axes in whatever order we like, it doesn't make a difference for the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "for j0,j1 in itertools.product(range(nfacet), range(nfacet)):\n",
    "    BF_F = prepare_facet(facet_2[j0,j1], 0)\n",
    "    for i0 in range(nsubgrid):\n",
    "        NMBF_F = extract_subgrid(BF_F, i0, 0)\n",
    "        NMBF_BF = prepare_facet(NMBF_F, 1)\n",
    "        for i1 in range(nsubgrid):\n",
    "            NMBF_NMBF[i0,i1,j0,j1] = extract_subgrid(NMBF_BF, i1, 1)\n",
    "print(time.time() - t, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "for j0,j1 in itertools.product(range(nfacet), range(nfacet)):\n",
    "    F_BF = prepare_facet(facet_2[j0,j1], 1)\n",
    "    for i1 in range(nsubgrid):\n",
    "        F_NMBF = extract_subgrid(F_BF, i1, 1)\n",
    "        BF_NMBF = prepare_facet(F_NMBF, 0)\n",
    "        for i0 in range(nsubgrid):\n",
    "            NMBF_NMBF[i0,i1,j0,j1] = extract_subgrid(BF_NMBF, i0, 0)\n",
    "print(time.time() - t, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = 16, 8\n",
    "err_mean = err_mean_img = 0\n",
    "for i0,i1 in itertools.product(range(nsubgrid), range(nsubgrid)):\n",
    "    approx = numpy.zeros((xM_size, xM_size), dtype=complex)\n",
    "    for j0,j1 in itertools.product(range(nfacet), range(nfacet)):\n",
    "        approx += numpy.roll(pad_mid(NMBF_NMBF[i0,i1,j0,j1], xM_size),\n",
    "                             (facet_off[j0]*xM_size//N, facet_off[j1]*xM_size//N), (0,1))\n",
    "    approx = extract_mid(ifft(approx), xA_size)\n",
    "    approx *= numpy.outer(subgrid_A[i0], subgrid_A[i1])\n",
    "    err_mean += numpy.abs(approx - subgrid_2[i0,i1])**2 / nsubgrid**2\n",
    "    err_mean_img += numpy.abs(fft(approx - subgrid_2[i0,i1]))**2 / nsubgrid**2\n",
    "pylab.imshow(numpy.log(numpy.sqrt(err_mean)) / numpy.log(10)); pylab.colorbar(); pylab.show()\n",
    "pylab.imshow(numpy.log(numpy.sqrt(err_mean_img)) / numpy.log(10)); pylab.colorbar(); pylab.show()\n",
    "print(\"RMSE:\", numpy.sqrt(numpy.mean(err_mean)), \"(image:\", numpy.sqrt(numpy.mean(err_mean_img)), \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(xs=(0,N), ys=(0,N))\n",
    "def test_accuracy(xs=252,ys=252):\n",
    "    subgrid_2 = numpy.empty((nsubgrid, nsubgrid, xA_size, xA_size), dtype=complex)\n",
    "    facet_2 = numpy.empty((nfacet, nfacet, yB_size, yB_size), dtype=complex)\n",
    "\n",
    "    #G_2 = numpy.exp(2j*numpy.pi*numpy.random.rand(N,N))*numpy.random.rand(N,N)/2\n",
    "    #FG_2 = fft(G_2)\n",
    "\n",
    "    FG_2 = numpy.zeros((N,N))\n",
    "    FG_2[ys,xs] = 1\n",
    "    G_2 = ifft(FG_2)\n",
    "\n",
    "    for i0,i1 in itertools.product(range(nsubgrid), range(nsubgrid)):\n",
    "        subgrid_2[i0,i1] = extract_mid(numpy.roll(G_2, (-subgrid_off[i0], -subgrid_off[i1]), (0,1)), xA_size)\n",
    "        subgrid_2[i0,i1] *= numpy.outer(subgrid_A[i0], subgrid_A[i1])\n",
    "    for j0,j1 in itertools.product(range(nfacet), range(nfacet)):\n",
    "        facet_2[j0,j1] = extract_mid(numpy.roll(FG_2, (-facet_off[j0], -facet_off[j1]), (0,1)), yB_size)\n",
    "        facet_2[j0,j1] *= numpy.outer(facet_B[j0], facet_B[j1])\n",
    "\n",
    "    NMBF_NMBF = numpy.empty((nsubgrid, nsubgrid, nfacet, nfacet, xM_yN_size, xM_yN_size), dtype=complex)\n",
    "    for j0,j1 in itertools.product(range(nfacet), range(nfacet)):\n",
    "        BF_F = prepare_facet(facet_2[j0,j1], 0)\n",
    "        BF_BF = prepare_facet(BF_F, 1)\n",
    "        for i0 in range(nsubgrid):\n",
    "            NMBF_BF = extract_subgrid(BF_BF, i0, 0)\n",
    "            for i1 in range(nsubgrid):\n",
    "                NMBF_NMBF[i0,i1,j0,j1] = extract_subgrid(NMBF_BF, i1, 1)\n",
    "\n",
    "    pylab.rcParams['figure.figsize'] = 16, 8\n",
    "    err_mean = err_mean_img = 0\n",
    "    for i0,i1 in itertools.product(range(nsubgrid), range(nsubgrid)):\n",
    "        approx = numpy.zeros((xM_size, xM_size), dtype=complex)\n",
    "        for j0,j1 in itertools.product(range(nfacet), range(nfacet)):\n",
    "            approx += numpy.roll(pad_mid(NMBF_NMBF[i0,i1,j0,j1], xM_size),\n",
    "                                 (facet_off[j0]*xM_size//N, facet_off[j1]*xM_size//N), (0,1))\n",
    "        approx = extract_mid(ifft(approx), xA_size)\n",
    "        approx *= numpy.outer(subgrid_A[i0], subgrid_A[i1])\n",
    "        err_mean += numpy.abs(approx - subgrid_2[i0,i1])**2 / nsubgrid**2\n",
    "        err_mean_img += numpy.abs(fft(approx - subgrid_2[i0,i1]))**2 / nsubgrid**2\n",
    "    #pylab.imshow(numpy.log(numpy.sqrt(err_mean)) / numpy.log(10)); pylab.colorbar(); pylab.show()\n",
    "    #pylab.imshow(numpy.log(numpy.sqrt(err_mean_img)) / numpy.log(10)); pylab.colorbar(); pylab.show()\n",
    "    print(\"RMSE:\", numpy.sqrt(numpy.mean(err_mean)), \"(image:\", numpy.sqrt(numpy.mean(err_mean_img)), \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "252/252: RMSE: 9.124879530036322e-16 (image: 1.751976869766974e-13 )\n",
    "252/1310: RMSE: 2.9755045512714923e-12 (image: 5.712968738441266e-10 )\n",
    "1307/1310: RMSE: 4.349813048259021e-12 (image: 8.35164105265732e-10 )\n",
    "1308/1310: RMSE: 4.726101167659007e-12 (image: 9.074114241905292e-10 )\n",
    "1309/1310: RMSE: 4.693531621846021e-12 (image: 9.011580713944359e-10 )\n",
    "1310/1310: RMSE: 4.208376099939205e-12 (image: 8.080082111883273e-10 )\n",
    "1311/1310: RMSE: 3.4437254455821605e-12 (image: 6.611952855517747e-10 )\n",
    "1308/1308: RMSE: 5.192955275658332e-12 (image: 9.970474129263996e-10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Degridding\n",
    "\n",
    "To use this for radio astronomy, our goal in this context is to (de)grid visibilities from subgrids. This uses very similar machinery - in fact, what we described so far can simply be re-expressed as gridding or degridding all points of a sub-grid using facets. Difference being that our method is a lot faster and requires less data movement.\n",
    "\n",
    "However, this does similarity does not actually buy us much: While for the recombination we consider the fields of view of facets, for gridding visibilities we are interested in the \"global\" field of view. Therefore we need a different grid correction and gridder that gets applied before and after we have done the combination, respectively.\n",
    "\n",
    "The full size of the considered image is fixed to $N$, therefore our effective image size is $2x_0N$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = 16, 4\n",
    "\n",
    "gc_alpha = 0; xGp = 4/N; gc_x0 = 0.3 # 0.125\n",
    "gc_support = int(2*xGp*N)\n",
    "print(\"parameter:\", numpy.pi*gc_support/2, 'support:', gc_support, \"x0:\", gc_x0)\n",
    "x0_size = int(N*gc_x0*2)\n",
    "gc_pswf = anti_aliasing_function(N, gc_alpha, numpy.pi*gc_support/2)\n",
    "gc = pad_mid(extract_mid(1 / gc_pswf, x0_size), N)\n",
    "pylab.semilogy(x0_size*coordinates(x0_size), numpy.abs(extract_mid(gc, x0_size))); pylab.legend([\"F[n]\"]);\n",
    "pylab.xlim((-N/1.8, N/1.8))\n",
    "mark_range(\"$x_0N$\", -gc_x0*N,gc_x0*N);\n",
    "mark_range(\"$N/2$\", -N/2,N/2); pylab.title(\"Grid correction\"); pylab.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we derive the new $\\mathcal F G$ that we are going to feed to the recombination algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_sources = [ (l,m,i) for l,m,i in sources\n",
    "                    if l >= -x0_size / 2 and l < x0_size / 2 and \\\n",
    "                       m >= -x0_size / 2 and m < x0_size / 2 ]\n",
    "print(f\"{len(cropped_sources)} / {len(sources)} sources still in view after cropping\")\n",
    "FG_2_gc = FG_2 * numpy.outer(gc, gc)\n",
    "G_2_gc = ifft(FG_2_gc)\n",
    "crop = pad_mid(numpy.ones(x0_size), N)\n",
    "G_2_cropped = ifft(FG_2 * numpy.outer(crop,crop))\n",
    "show_image(numpy.log(numpy.maximum(1e-15, numpy.abs(fft(G_2_cropped)))) / numpy.log(10), \"FG_2_cropped\", N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test base performance of gridder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dft(sources, theta, u,v,w):\n",
    "    actual = 0\n",
    "    for l, m, i in sources:\n",
    "        n = 1 - numpy.sqrt(1 - (l/N*theta)**2 - (m/N*theta)**2)\n",
    "        actual += i * numpy.exp(2j * numpy.pi * (u * l + v * m + w * n))\n",
    "    return actual / N / N\n",
    "\n",
    "@interact(iu=(0, N, 0.1),iv=(0, N, 0.1), theta=(0,2,0.01), w=(-2000,2000,0.5))\n",
    "def test_degrid_accuracy(iu,iv=N/2,w=1000, theta=0.3):\n",
    "    u = (iu - N//2) / N; v = (iv - N//2) / N\n",
    "    actual = dft(cropped_sources,theta,u,v,w)\n",
    "    G_2_gc_= G_2_gc\n",
    "    if w != 0:\n",
    "        l,m = numpy.meshgrid(theta*coordinates(N), theta*coordinates(N))\n",
    "        n = 1 - numpy.sqrt(1 - l**2 - m**2)\n",
    "        G_2_gc_ = ifft(fft(G_2_gc_) * numpy.exp(2j * numpy.pi * w * n))\n",
    "    deg = conv_predict(N, 1, numpy.array([(u,v,0)]), None, G_2_gc_, kernel)[0]\n",
    "    print(\"actual:       \", actual)\n",
    "    print(\"degridded:    \", deg)\n",
    "    print(\"degrid error: \", numpy.abs(deg-actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which in turn leads to new facets. Note how the grid correction pattern is clearly larger than any individual facet.\n",
    "\n",
    "The other thing to notice here is that due to the grid correction margin a significant portion of the image is now zero, which translates to entire facets being zero. Due to the linearity of the method this means we could simply skip those. For the purpose of this notebook we do not use this, but it is a good optimisation to keep in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgrid_2 = numpy.empty((nsubgrid, nsubgrid, xA_size, xA_size), dtype=complex)\n",
    "facet_2 = numpy.empty((nfacet, nfacet, yB_size, yB_size), dtype=complex)\n",
    "for i0,i1 in itertools.product(range(nsubgrid), range(nsubgrid)):\n",
    "    subgrid_2[i0,i1] = extract_mid(numpy.roll(G_2_gc, (-subgrid_off[i0], -subgrid_off[i1]), (0,1)), xA_size)\n",
    "    subgrid_2[i0,i1] *= numpy.outer(subgrid_A[i0], subgrid_A[i1])\n",
    "fig = pylab.figure(figsize=(32,32))\n",
    "for j0,j1 in itertools.product(range(nfacet), range(nfacet)):\n",
    "    facet_2[j0,j1] = extract_mid(numpy.roll(FG_2_gc, (-facet_off[j0], -facet_off[j1]), (0,1)), yB_size)\n",
    "    facet_2[j0,j1] *= numpy.outer(facet_B[j0], facet_B[j1])\n",
    "    show_image(numpy.log(numpy.maximum(1e-15, numpy.abs(facet_2[j0,j1]))) / numpy.log(10),\n",
    "               \"facet_%d%d\" % (j0,j1), N, axes=fig.add_subplot(nfacet,nfacet,j1+(nfacet-j0-1)*nfacet+1),\n",
    "              norm=(-15,8))\n",
    "pylab.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recombination algorithm again, using the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NMBF_NMBF = numpy.empty((nsubgrid, nsubgrid, nfacet, nfacet, xM_yN_size, xM_yN_size), dtype=complex)\n",
    "for j0,j1 in itertools.product(range(nfacet), range(nfacet)):\n",
    "    F_BF = prepare_facet(facet_2[j0,j1], 1)\n",
    "    for i1 in range(nsubgrid):\n",
    "        F_NMBF = extract_subgrid(F_BF, i1, 1)\n",
    "        BF_NMBF = prepare_facet(F_NMBF, 0)\n",
    "        for i0 in range(nsubgrid):\n",
    "            NMBF_NMBF[i0,i1,j0,j1] = extract_subgrid(BF_NMBF, i0, 0)\n",
    "\n",
    "from pylru import lrudecorator\n",
    "@lrudecorator(100)\n",
    "def make_approx_subgrid(i0,i1):\n",
    "    approx = numpy.zeros((xM_size, xM_size), dtype=complex)\n",
    "    for j0,j1 in itertools.product(range(nfacet), range(nfacet)):\n",
    "        approx += numpy.roll(pad_mid(NMBF_NMBF[i0,i1,j0,j1], xM_size),\n",
    "                             (facet_off[j0]*xM_size//N, facet_off[j1]*xM_size//N), (0,1))\n",
    "    # Extract region that is set in subgrid for comparison\n",
    "    approx_compare = extract_mid(ifft(approx), xA_size)\n",
    "    approx_compare *= numpy.outer(subgrid_A[i0], subgrid_A[i1])\n",
    "    rmse = numpy.sqrt(numpy.mean(numpy.abs(approx_compare - subgrid_2[i0,i1])**2 / nsubgrid**2))\n",
    "    # Return full approximation. We degrid from it, so bounds don't matter\n",
    "    return ifft(approx), rmse / numpy.mean(numpy.abs(approx_compare))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain visibilities at non-integer positions we need an oversampled gridding function, as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = 2**14\n",
    "print(\"grid support:\", gc_support)\n",
    "print(\"oversampling:\", oversample)\n",
    "kernel = kernel_oversample(gc_pswf, oversample, gc_support).real\n",
    "kernel /= numpy.sum(kernel[0])\n",
    "r = numpy.arange(-oversample*(gc_support//2), oversample*((gc_support+1)//2)) / oversample\n",
    "pylab.semilogy(r, numpy.transpose(kernel).flatten().real); mark_range(\"$Nx_G$\", -N*xGp,N*xGp);\n",
    "pylab.title(\"Gridding kernel (oversampled x%d)\" % oversample); pylab.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(iu=(0, N, 0.1),iv=(0, N, 0.1), theta=(0,2,0.01), w=(-2000,2000,0.5))\n",
    "def test_degrid_accuracy(iu,iv=N/2,w=1000, theta=0.3, show_subgrid=True):\n",
    "    u = (iu - N//2) / N; v = (iv - N//2) / N\n",
    "    su = numpy.sum((iu+N//2)%N >= subgrid_border) % nsubgrid\n",
    "    sv = numpy.sum((iv+N//2)%N >= subgrid_border) % nsubgrid\n",
    "    siu = iu + xA_size//2-(subgrid_off[su] + N//2) % N\n",
    "    siv = iv + xA_size//2-(subgrid_off[sv] + N//2) % N\n",
    "    \n",
    "    # Predict from grid\n",
    "    G_2_gc_= G_2_gc\n",
    "    if w != 0:\n",
    "        l,m = numpy.meshgrid(theta*coordinates(N), theta*coordinates(N))\n",
    "        n = 1 - numpy.sqrt(1 - l**2 - m**2)\n",
    "        G_2_gc_ = ifft(fft(G_2_gc_) * numpy.exp(2j * numpy.pi * w * n))\n",
    "    deg = conv_predict(N, 1, numpy.array([(u,v,0)]), None, G_2_gc_, kernel)[0]\n",
    "    actual = dft(cropped_sources, theta, u,v,w)\n",
    "    print(\"actual:       \", actual)\n",
    "    print(\"degridded:    \", deg)\n",
    "    print(\"degrid error: \", numpy.abs(deg-actual))\n",
    "\n",
    "    true_subgrid =  extract_mid(numpy.roll(G_2_gc_, (-subgrid_off[sv], -subgrid_off[su]), (0,1)), xM_size)\n",
    "\n",
    "    approx_subgrid, rmse = make_approx_subgrid(sv, su)\n",
    "    if w != 0:\n",
    "        l,m = numpy.meshgrid(theta*coordinates(xM_size), theta*coordinates(xM_size))\n",
    "        n = 1 - numpy.sqrt(1 - l**2 - m**2)\n",
    "        approx_subgrid = ifft(fft(approx_subgrid) * numpy.exp(2j * numpy.pi * w * n))\n",
    "    print(\"subgrid:       (%d/%d), rmse: %g\" % (su, sv, rmse))\n",
    "    \n",
    "    sou = (((subgrid_off[su] + N//2) % N) - N//2) / N\n",
    "    sov = (((subgrid_off[sv] + N//2) % N) - N//2) / N\n",
    "    deg_ap = conv_predict(N, 2*xM, numpy.array([(u-sou,v-sov,0)]), None, approx_subgrid, kernel)[0]\n",
    "    print(\"recomb+degrid:\", deg_ap);\n",
    "    print(\"recomb error: \", numpy.abs(deg_ap-deg))\n",
    "    print(\"total error:  \", numpy.abs(deg_ap-actual))\n",
    "    print(\"w theta^2:    \", w * theta**2)\n",
    "    if show_subgrid:\n",
    "\n",
    "        lam = xM_size / N\n",
    "        uv_lower, uv_upper = coordinateBounds(xM_size)\n",
    "        uv_lower = (uv_lower-1./xM_size/2)*lam\n",
    "        uv_upper = (uv_upper+1./xM_size/2)*lam\n",
    "        extent = (uv_lower+sou, uv_upper+sou,\n",
    "                  uv_lower+sov, uv_upper+sov)\n",
    "        \n",
    "        fig = pylab.figure(figsize=(16,16))\n",
    "        ax = fig.add_subplot(131)\n",
    "        ax.imshow(approx_subgrid.real,extent=extent)\n",
    "        \n",
    "        #Fg = numpy.exp(2.j * numpy.pi * w * numpy.sqrt(1 - (coordinates(res) * theta)**2))\n",
    "        Gn = numpy.abs(ifft(pswfs[W]**2))\n",
    "        p = find_x_sorted_logsmooth(-coordinates(res), Gn, err_base[W])\n",
    "        \n",
    "        ov = 4\n",
    "        print('effective theta:', theta*gc_x0)\n",
    "        Fg = pad_mid(numpy.exp(2.j * numpy.pi * w * numpy.sqrt(1 - (coordinates(res*ov//2)*theta*gc_x0*2)**2)),\n",
    "                     res*ov)\n",
    "        Fn = test_pswf(W, ov)\n",
    "        Gn = numpy.abs(ifft(Fg * Fn**2))\n",
    "        #pylab.semilogy(numpy.abs(Fg*Fn**2))\n",
    "        p = find_x_sorted_logsmooth(-coordinates(res*ov), Gn, err_base[W])\n",
    "        print('p=', p)\n",
    "\n",
    "        degrid_patch = lambda: patches.Rectangle((u-gc_support//2/N, v-gc_support//2/N),\n",
    "                                                  gc_support/N, gc_support/N, fill=False)\n",
    "        print(W, W2s_map[W])\n",
    "        xA_size_ = xM_size - p * ov * res / yN_size * N #W2s_map[W] / yN_size * N\n",
    "        xA_patch = lambda: patches.Rectangle((sou-xA_size_//2/N,sov-xA_size_//2/N),\n",
    "                                             (xA_size_-1)/N, (xA_size_-1)/N, fill=False)\n",
    "        ax.add_patch(degrid_patch())\n",
    "        if xA_size_ > 0: ax.add_patch(xA_patch())\n",
    "        ax2 = fig.add_subplot(132)\n",
    "        im2 = ax2.imshow(numpy.log(numpy.abs(approx_subgrid - true_subgrid)) / numpy.log(10), extent=extent)\n",
    "        ax2.add_patch(degrid_patch())\n",
    "        if xA_size_ > 0: ax2.add_patch(xA_patch())\n",
    "        fig.colorbar(im2, shrink=.4)\n",
    "        ax3 = fig.add_subplot(133)\n",
    "        AG = extract_mid(approx_subgrid - true_subgrid, int(xA_size_))\n",
    "        print('subgrid area rmse', numpy.sqrt(numpy.mean(numpy.abs(AG)**2)))\n",
    "        im3 = ax3.imshow(numpy.log(numpy.abs(AG)) / numpy.log(10)\n",
    "                         , extent=extent)\n",
    "        pylab.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import h5py\n",
    "\n",
    "out_prefix = \"../../data/grid/T05b_\"\n",
    "@interact_manual\n",
    "def export_test(test_path = \"../../data/grid/T06_pswf_%s.in\" % cfg_name):\n",
    "\n",
    "    with h5py.File(out_prefix + \"in.h5\",'w') as f:\n",
    "        f['pswf'] = numpy.fft.ifftshift(pswf)\n",
    "        f['sepkern/kern'] = kernel\n",
    "        for j0,j1 in itertools.product(range(nfacet), range(nfacet)):\n",
    "            f[\"j0=%d/j1=%d/facet\" % (j0,j1)] = numpy.fft.ifftshift(facet_2[j0,j1])\n",
    "            for i0,i1 in itertools.product(range(nsubgrid), range(nsubgrid)):\n",
    "                f['i0=%d/i1=%d/j0=%d/j1=%d/nmbf' % (i0,i1,j0,j1)] = \\\n",
    "                    numpy.fft.ifftshift(NMBF_NMBF[i0,i1,j0,j1])\n",
    "        for i0,i1 in itertools.product(range(nsubgrid), range(nsubgrid)):\n",
    "            #f[\"i0=%d/i1=%d/subgrid\" % (i0,i1)] = numpy.fft.ifftshift(subgrid_2[i0, i1])\n",
    "            f[\"i0=%d/i1=%d/approx\" % (i0,i1)] = numpy.fft.ifftshift(make_approx_subgrid(i0, i1)[0])\n",
    "        for sv in range(nsubgrid):\n",
    "            for su in range(nsubgrid):\n",
    "                # Write\n",
    "                f['i0=%d/i1=%d/uvw' % (sv,su)] = uvws[sel_sg[sv,su]]\n",
    "                f['i0=%d/i1=%d/uvw_subgrid' % (sv,su)] = uvw_sg[sv,su]\n",
    "                f['i0=%d/i1=%d/vis' % (sv,su)] = deg[sel_sg[sv,su]]\n",
    "                #f['i0=%d/i1=%d/vis_approx' % (sv,su)] = deg_ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "@interact_manual\n",
    "def export_kernel(kernel_path = \"../../data/grid/kernel_%d_%g.in\" % (gc_support, gc_x0)):\n",
    "    with h5py.File(kernel_path,'w') as f:\n",
    "        f['sepkern/corr'] = numpy.fft.ifftshift(gc_pswf)\n",
    "        f['sepkern/kern'] = kernel\n",
    "        f['sepkern/x0'] = gc_x0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
